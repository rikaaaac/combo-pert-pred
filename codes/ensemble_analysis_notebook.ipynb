{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models analysis\n",
    "\n",
    "This notebook provides interactive analysis of pre-trained ensemble models for combinatorial perturbation prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ensemble'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-572791917.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ensemble framework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnsemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mensemble_analyze\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnsembleAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ensemble'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ensemble framework\n",
    "from ensemble import Ensemble\n",
    "from ensemble_analyze import EnsembleAnalyzer\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCLAMBDA_REPO = '/Users/rikac/Documents/scLAMBDA'\n",
    "DATA_DIR = '/Users/rikac/Documents/ml_stats/combo-pert-pred/data'\n",
    "MODEL_DIR = '/Users/rikac/Documents/ml_stats/combo-pert-pred/models'\n",
    "\n",
    "# model paths\n",
    "GEARS_MODEL_DIR = f'{MODEL_DIR}/gears_model'\n",
    "GEARS_DATA_PATH = f'{MODEL_DIR}/gears_data'\n",
    "SCLAMBDA_MODEL_PATH = f'{MODEL_DIR}/sclambda_model'\n",
    "SCLAMBDA_ADATA_PATH = f'{DATA_DIR}/norman_perturbseq_preprocessed_hvg_filtered.h5ad'\n",
    "SCLAMBDA_EMBEDDINGS_PATH = f'{DATA_DIR}/GPT_3_5_gene_embeddings_3-large.pickle'\n",
    "NORMAN_DATA_PATH = f'{DATA_DIR}/norman_perturbseq_preprocessed_hvg_filtered.h5ad'\n",
    "\n",
    "OUTPUT_DIR = '../ensemble_results'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pre-trained models\n",
    "\n",
    "This loads:\n",
    "- pre-trained GEARS model\n",
    "- pre-trained scLAMBDA model\n",
    "- baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ensemble models...\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Ensemble' object has no attribute 'load_models_with_device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m sclambda_checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSCLAMBDA_MODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ckpt.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# load all models\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_models_with_device\u001b[49m(\n\u001b[1;32m     12\u001b[0m     gears_model_dir\u001b[38;5;241m=\u001b[39mGEARS_MODEL_DIR,\n\u001b[1;32m     13\u001b[0m     gears_data_path\u001b[38;5;241m=\u001b[39mGEARS_DATA_PATH,\n\u001b[1;32m     14\u001b[0m     gears_data_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorman\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m     sclambda_model_or_path\u001b[38;5;241m=\u001b[39msclambda_checkpoint,\n\u001b[1;32m     16\u001b[0m     sclambda_adata_path\u001b[38;5;241m=\u001b[39mSCLAMBDA_ADATA_PATH,\n\u001b[1;32m     17\u001b[0m     sclambda_embeddings_path\u001b[38;5;241m=\u001b[39mSCLAMBDA_EMBEDDINGS_PATH,\n\u001b[1;32m     18\u001b[0m     norman_data_path\u001b[38;5;241m=\u001b[39mNORMAN_DATA_PATH\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m All models loaded successfully!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of genes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ensemble\u001b[38;5;241m.\u001b[39mgene_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Ensemble' object has no attribute 'load_models_with_device'"
     ]
    }
   ],
   "source": [
    "print('Loading ensemble models...')\n",
    "print('=' * 70)\n",
    "\n",
    "# initialize ensemble\n",
    "ensemble = Ensemble(sclambda_repo_path=SCLAMBDA_REPO)\n",
    "\n",
    "# need to change to cpu for macbook\n",
    "sclambda_checkpoint = torch.load(f'{SCLAMBDA_MODEL_PATH}/ckpt.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# load all models\n",
    "ensemble.load_models_with_device(\n",
    "    gears_model_dir=GEARS_MODEL_DIR,\n",
    "    gears_data_path=GEARS_DATA_PATH,\n",
    "    gears_data_name='norman',\n",
    "    sclambda_model_or_path=sclambda_checkpoint,\n",
    "    sclambda_adata_path=SCLAMBDA_ADATA_PATH,\n",
    "    sclambda_embeddings_path=SCLAMBDA_EMBEDDINGS_PATH,\n",
    "    norman_data_path=NORMAN_DATA_PATH\n",
    ")\n",
    "\n",
    "print('\\n All models loaded successfully!')\n",
    "print(f'Number of genes: {len(ensemble.gene_names)}')\n",
    "print(f'Single perturbations: {ensemble.X_single.shape[0]}')\n",
    "print(f'Combo perturbations: {ensemble.X_combo.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data Splits\n",
    "\n",
    "Split data for evaluation (GEARS-style: train on singles + some combos, test on held-out combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating data splits...\")\n",
    "\n",
    "splits = ensemble.data_processor.create_combo_splits(\n",
    "    X_single=ensemble.X_single,\n",
    "    y_single=ensemble.y_single,\n",
    "    X_combo=ensemble.X_combo,\n",
    "    y_combo=ensemble.y_combo,\n",
    "    combo_test_ratio=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit created:\")\n",
    "print(f\"  Training samples: {splits['X_train'].shape[0]} (includes all {ensemble.X_single.shape[0]} singles)\")\n",
    "print(f\"  Validation samples: {splits['X_val'].shape[0]}\")\n",
    "print(f\"  Test samples: {splits['X_test'].shape[0]}\")\n",
    "print(f\"  Split type: {splits['split_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Analyzer\n",
    "\n",
    "Create the analyzer object that will handle all evaluations and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing analyzer...\")\n",
    "\n",
    "analyzer = EnsembleAnalyzer(\n",
    "    ensemble=ensemble, \n",
    "    splits=splits, \n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Analyzer initialized\")\n",
    "print(f\"   Output directory: {analyzer.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Individual Models\n",
    "\n",
    "Get predictions from all models and compute performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "metrics = analyzer.evaluate_individual_models()\n",
    "\n",
    "# Create a nice summary table\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df = metrics_df.round(6)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(metrics_df)\n",
    "\n",
    "# Highlight best performers\n",
    "print(\"\\nüèÜ Best Performers:\")\n",
    "print(f\"   Lowest MSE: {metrics_df['mse'].idxmin()} ({metrics_df['mse'].min():.6f})\")\n",
    "print(f\"   Highest Pearson r: {metrics_df['pearson_r'].idxmax()} ({metrics_df['pearson_r'].max():.6f})\")\n",
    "print(f\"   Highest R¬≤: {metrics_df['r2'].idxmax()} ({metrics_df['r2'].max():.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison Visualization\n",
    "\n",
    "Generate comprehensive comparison plots across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_model_comparison()\n",
    "\n",
    "# Display the saved plot\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=f'{OUTPUT_DIR}/model_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Uncertainty Distribution Analysis\n",
    "\n",
    "Analyze epistemic uncertainty across test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sample_uncertainty = analyzer.plot_uncertainty_distribution()\n",
    "\n",
    "# Display the saved plot\n",
    "display(Image(filename=f'{OUTPUT_DIR}/uncertainty_distribution.png'))\n",
    "\n",
    "# Print uncertainty statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UNCERTAINTY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mean uncertainty: {np.mean(per_sample_uncertainty):.4f}\")\n",
    "print(f\"Median uncertainty: {np.median(per_sample_uncertainty):.4f}\")\n",
    "print(f\"Std uncertainty: {np.std(per_sample_uncertainty):.4f}\")\n",
    "print(f\"Min uncertainty: {np.min(per_sample_uncertainty):.4f}\")\n",
    "print(f\"Max uncertainty: {np.max(per_sample_uncertainty):.4f}\")\n",
    "print(f\"95th percentile: {np.percentile(per_sample_uncertainty, 95):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Uncertainty Calibration\n",
    "\n",
    "Check if high uncertainty correlates with high prediction error (indicates well-calibrated uncertainty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_uncertainty_vs_error()\n",
    "\n",
    "# Display the saved plot\n",
    "display(Image(filename=f'{OUTPUT_DIR}/uncertainty_vs_error.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Summary Report\n",
    "\n",
    "Save comprehensive text and JSON summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.save_summary()\n",
    "\n",
    "# Display the summary\n",
    "with open(f'{OUTPUT_DIR}/summary.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Exploration: Make Custom Predictions\n",
    "\n",
    "Test the ensemble on specific perturbations of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: predict a specific combo perturbation\n",
    "# Modify gene names below to test specific perturbations\n",
    "\n",
    "gene1 = 'CBL'  # Replace with gene of interest\n",
    "gene2 = 'CNN1'  # Replace with gene of interest\n",
    "\n",
    "# Create perturbation vector\n",
    "X_custom = np.zeros((1, len(ensemble.gene_names)))\n",
    "\n",
    "if gene1 in ensemble.gene_names and gene2 in ensemble.gene_names:\n",
    "    idx1 = ensemble.gene_names.index(gene1)\n",
    "    idx2 = ensemble.gene_names.index(gene2)\n",
    "    X_custom[0, [idx1, idx2]] = 1.0\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_mean, uncertainties, individual_preds = ensemble.predict_ensemble(X_custom)\n",
    "    \n",
    "    print(f\"\\nPrediction for: {gene1} + {gene2}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nIndividual model predictions (mean across genes):\")\n",
    "    for model_name, preds in individual_preds.items():\n",
    "        print(f\"  {model_name:12s}: {np.mean(preds[0]):.4f}\")\n",
    "    \n",
    "    print(f\"\\nEnsemble prediction (mean): {np.mean(pred_mean[0]):.4f}\")\n",
    "    print(f\"Total uncertainty: {np.sum(uncertainties[0]):.4f}\")\n",
    "    print(f\"Mean uncertainty per gene: {np.mean(uncertainties[0]):.6f}\")\n",
    "    \n",
    "    # Visualize prediction distribution across genes\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Predicted expression changes\n",
    "    top_genes = 20\n",
    "    sorted_idx = np.argsort(np.abs(pred_mean[0]))[-top_genes:]\n",
    "    \n",
    "    axes[0].barh(range(top_genes), pred_mean[0][sorted_idx], \n",
    "                color=['red' if x < 0 else 'blue' for x in pred_mean[0][sorted_idx]])\n",
    "    axes[0].set_yticks(range(top_genes))\n",
    "    axes[0].set_yticklabels([f'Gene {i}' for i in sorted_idx], fontsize=8)\n",
    "    axes[0].set_xlabel('Predicted Expression Change', fontweight='bold')\n",
    "    axes[0].set_title(f'Top {top_genes} Affected Genes: {gene1}+{gene2}', fontweight='bold')\n",
    "    axes[0].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Uncertainty per gene\n",
    "    sorted_unc_idx = np.argsort(uncertainties[0])[-top_genes:]\n",
    "    \n",
    "    axes[1].barh(range(top_genes), uncertainties[0][sorted_unc_idx], color='orange')\n",
    "    axes[1].set_yticks(range(top_genes))\n",
    "    axes[1].set_yticklabels([f'Gene {i}' for i in sorted_unc_idx], fontsize=8)\n",
    "    axes[1].set_xlabel('Epistemic Uncertainty', fontweight='bold')\n",
    "    axes[1].set_title(f'Top {top_genes} Most Uncertain Genes', fontweight='bold')\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"Error: One or both genes not found in dataset\")\n",
    "    print(f\"Available genes: {ensemble.gene_names[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Experiment Recommendations\n",
    "\n",
    "Identify high-priority experiments based on epistemic uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all test set predictions with uncertainties\n",
    "_, test_uncertainties, test_individual_preds = ensemble.predict_ensemble(splits['X_test'])\n",
    "test_uncertainty_scores = np.sum(test_uncertainties, axis=1)\n",
    "\n",
    "# Find top uncertain samples\n",
    "n_recommend = 10\n",
    "top_uncertain_idx = np.argsort(test_uncertainty_scores)[-n_recommend:]\n",
    "\n",
    "print(f\"\\nüî¨ TOP {n_recommend} RECOMMENDED EXPERIMENTS (Highest Uncertainty)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recommendations_data = []\n",
    "\n",
    "for rank, idx in enumerate(reversed(top_uncertain_idx), 1):\n",
    "    perturbation = splits['X_test'][idx]\n",
    "    perturbed_gene_idx = np.where(perturbation > 0)[0]\n",
    "    perturbed_genes = [ensemble.gene_names[i] for i in perturbed_gene_idx]\n",
    "    uncertainty = test_uncertainty_scores[idx]\n",
    "    \n",
    "    # Get model predictions\n",
    "    model_preds = {}\n",
    "    for model_name, preds in test_individual_preds.items():\n",
    "        model_preds[model_name] = np.mean(preds[idx])\n",
    "    \n",
    "    print(f\"\\n{rank:2d}. Perturbation: {' + '.join(perturbed_genes)}\")\n",
    "    print(f\"    Uncertainty score: {uncertainty:.4f}\")\n",
    "    print(f\"    Model predictions (mean expression):\")\n",
    "    for model_name, pred_val in model_preds.items():\n",
    "        print(f\"      {model_name:12s}: {pred_val:7.4f}\")\n",
    "    print(f\"    ‚Üí Model disagreement = high learning potential\")\n",
    "    \n",
    "    recommendations_data.append({\n",
    "        'rank': rank,\n",
    "        'genes': ' + '.join(perturbed_genes),\n",
    "        'uncertainty': uncertainty,\n",
    "        **{f'{m}_pred': model_preds[m] for m in model_preds.keys()}\n",
    "    })\n",
    "\n",
    "# Create recommendations dataframe\n",
    "recommendations_df = pd.DataFrame(recommendations_data)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATIONS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(recommendations_df.round(4))\n",
    "\n",
    "# Save recommendations\n",
    "recommendations_df.to_csv(f'{OUTPUT_DIR}/experiment_recommendations.csv', index=False)\n",
    "print(f\"\\n‚úÖ Recommendations saved to: {OUTPUT_DIR}/experiment_recommendations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Agreement Analysis\n",
    "\n",
    "Examine how well different models agree with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise correlations between models\n",
    "models = ['gears', 'sclambda', 'mean', 'additive', 'ensemble']\n",
    "\n",
    "# Get ensemble prediction\n",
    "ensemble_pred = np.mean(np.stack([\n",
    "    analyzer.predictions['gears'],\n",
    "    analyzer.predictions['sclambda'],\n",
    "    analyzer.predictions['mean'],\n",
    "    analyzer.predictions['additive']\n",
    "], axis=0), axis=0)\n",
    "\n",
    "all_preds = {\n",
    "    'gears': analyzer.predictions['gears'],\n",
    "    'sclambda': analyzer.predictions['sclambda'],\n",
    "    'mean': analyzer.predictions['mean'],\n",
    "    'additive': analyzer.predictions['additive'],\n",
    "    'ensemble': ensemble_pred\n",
    "}\n",
    "\n",
    "# Compute correlation matrix\n",
    "n_models = len(models)\n",
    "corr_matrix = np.zeros((n_models, n_models))\n",
    "\n",
    "for i, model1 in enumerate(models):\n",
    "    for j, model2 in enumerate(models):\n",
    "        corr, _ = pearsonr(all_preds[model1].flatten(), all_preds[model2].flatten())\n",
    "        corr_matrix[i, j] = corr\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(corr_matrix, cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(n_models))\n",
    "ax.set_yticks(np.arange(n_models))\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_yticklabels(models)\n",
    "\n",
    "# Rotate the tick labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(n_models):\n",
    "    for j in range(n_models):\n",
    "        text = ax.text(j, i, f'{corr_matrix[i, j]:.3f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "ax.set_title(\"Model Agreement: Pairwise Correlations\", fontsize=14, fontweight='bold', pad=20)\n",
    "fig.colorbar(im, ax=ax, label='Pearson Correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/model_agreement.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Model Agreement Analysis:\")\n",
    "print(f\"   Average pairwise correlation: {np.mean(corr_matrix[np.triu_indices(n_models, k=1)]):.3f}\")\n",
    "print(f\"   Most similar models: \", end=\"\")\n",
    "max_corr_idx = np.unravel_index(np.argmax(corr_matrix + np.eye(n_models) * -10), corr_matrix.shape)\n",
    "print(f\"{models[max_corr_idx[0]]} vs {models[max_corr_idx[1]]} (r={corr_matrix[max_corr_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Conclusions\n",
    "\n",
    "Key findings from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ENSEMBLE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best model\n",
    "best_model = metrics_df['mse'].idxmin()\n",
    "best_mse = metrics_df['mse'].min()\n",
    "ensemble_mse = metrics_df.loc['ensemble', 'mse']\n",
    "ensemble_r2 = metrics_df.loc['ensemble', 'r2']\n",
    "\n",
    "print(f\"\\nüìä Overall Performance:\")\n",
    "print(f\"   Best individual model: {best_model} (MSE: {best_mse:.6f})\")\n",
    "print(f\"   Ensemble performance: MSE: {ensemble_mse:.6f}, R¬≤: {ensemble_r2:.6f}\")\n",
    "\n",
    "if ensemble_mse < best_mse:\n",
    "    improvement = (best_mse - ensemble_mse) / best_mse * 100\n",
    "    print(f\"   ‚ú® Ensemble improves over best individual by {improvement:.2f}%\")\n",
    "\n",
    "print(f\"\\nüî¨ Uncertainty Insights:\")\n",
    "print(f\"   Mean uncertainty: {np.mean(per_sample_uncertainty):.4f}\")\n",
    "print(f\"   High-priority experiments identified: {n_recommend}\")\n",
    "\n",
    "# Compute uncertainty-error correlation\n",
    "y_test = splits['y_test']\n",
    "per_sample_error = np.mean((ensemble_pred - y_test) ** 2, axis=1)\n",
    "unc_err_corr, p_val = pearsonr(test_uncertainty_scores, per_sample_error)\n",
    "\n",
    "print(f\"\\nüìà Calibration:\")\n",
    "print(f\"   Uncertainty-error correlation: {unc_err_corr:.3f} (p={p_val:.2e})\")\n",
    "if unc_err_corr > 0.3 and p_val < 0.05:\n",
    "    print(f\"   ‚úÖ Well-calibrated: High uncertainty ‚Üí High error\")\n",
    "elif unc_err_corr > 0 and p_val < 0.05:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderately calibrated\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Uncertainty may need recalibration\")\n",
    "\n",
    "print(f\"\\nüíæ Generated Files:\")\n",
    "print(f\"   - {OUTPUT_DIR}/summary.txt\")\n",
    "print(f\"   - {OUTPUT_DIR}/metrics.json\")\n",
    "print(f\"   - {OUTPUT_DIR}/model_comparison.png\")\n",
    "print(f\"   - {OUTPUT_DIR}/uncertainty_distribution.png\")\n",
    "print(f\"   - {OUTPUT_DIR}/uncertainty_vs_error.png\")\n",
    "print(f\"   - {OUTPUT_DIR}/model_agreement.png\")\n",
    "print(f\"   - {OUTPUT_DIR}/experiment_recommendations.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Analysis notebook complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
